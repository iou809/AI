{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "1Khwg0tu7g8B",
        "CpG-X8cN7efL",
        "YU1TsqVo79nZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# mnist 호출"
      ],
      "metadata": {
        "id": "1Khwg0tu7g8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mnist import load_mnist\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)"
      ],
      "metadata": {
        "id": "wBNJFwpqEHdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "CpG-X8cN7efL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SGD:\n",
        "\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "    \"\"\"모멘텀 SGD\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
        "            params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "\n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
      ],
      "metadata": {
        "id": "KKzMcleY7dWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer"
      ],
      "metadata": {
        "id": "YU1TsqVo79nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원\n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "  # coding: utf-8\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "metadata": {
        "id": "inkRWPa378rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "9H-dyR528GhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNoOos1x60Ki"
      },
      "outputs": [],
      "source": [
        "class SimpleConvNet:\n",
        "    \"\"\"\n",
        "    다음과 같은 CNN을 구성한다.\n",
        "    → Conv → ReLU → Pooling → Affine → ReLU → Affine → Softmax →\n",
        "    전체 구현은 simple_convnet.py 참고\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
        "                             'pad': 0, 'stride': 1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        # 초기화 인수로 주어진 하이퍼파라미터를 딕셔너리에서 꺼내고 출력 크기를 계산한다.\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
        "            filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
        "                               (conv_output_size/2))\n",
        "\n",
        "        # 1층의 합성곱 계층과 2, 3층의 완전연결 계층의 가중치와 편향 생성\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # CNN을 구성하는 계층을 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
        "                                           self.params['b1'],\n",
        "                                           conv_param['stride'],\n",
        "                                           conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"추론을 수행\"\"\"\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실함수 값 계산\"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1:\n",
        "            t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"오차역전파법으로 기울기를 구함\"\"\"\n",
        "        # 순전파\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # 역전파\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'] = self.layers['Conv1'].dW\n",
        "        grads['b1'] = self.layers['Conv1'].db\n",
        "        grads['W2'] = self.layers['Affine1'].dW\n",
        "        grads['b2'] = self.layers['Affine1'].db\n",
        "        grads['W3'] = self.layers['Affine2'].dW\n",
        "        grads['b3'] = self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer2:\n",
        "\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01},\n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "\n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "\n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))"
      ],
      "metadata": {
        "id": "H5Nn_rl-7V2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=15, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "max_epochs = 20\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L6FrXn04B5wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실험"
      ],
      "metadata": {
        "id": "CwCIVQS5G7Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='adam', optimizer_param={'lr':0.01},\n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimizer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprop':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "\n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "\n",
        "        self.current_iter += 1\n",
        "\n",
        "        # 테스트 정확도를 매번 확인하여 99% 이상일 경우 학습 중단\n",
        "        x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "        if self.evaluate_sample_num_per_epoch is not None:\n",
        "            t = self.evaluate_sample_num_per_epoch\n",
        "            x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "        test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "        if test_acc >= 0.99:\n",
        "            if self.verbose:\n",
        "                print(f\"=== Iteration:{self.current_iter}, train loss:{loss}, test acc:{test_acc} ===\")\n",
        "                print(f\"=== Test accuracy reached 99%. Stopping training. ===\")\n",
        "            return True  # 학습 중단 신호 반환\n",
        "\n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            if self.evaluate_sample_num_per_epoch is not None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "\n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"=== epoch:{self.current_epoch}, train loss:{loss}, train acc:{train_acc}, test acc:{test_acc} ===\")\n",
        "\n",
        "        return False  # 학습 계속 진행\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            if self.train_step():\n",
        "                break  # 학습 중단 신호가 반환되면 루프 탈출\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))\n"
      ],
      "metadata": {
        "id": "2TCWaCxqGtRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=15, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "max_epochs = 20\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "SDyVYNChHC20",
        "outputId": "0bd2eeac-60a4-40ef-9239-362f6a54f753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== epoch:1, train loss:0.21909241790206851, train acc:0.925, test acc:0.924 ===\n",
            "=== epoch:2, train loss:0.13883125817998163, train acc:0.958, test acc:0.955 ===\n",
            "=== epoch:3, train loss:0.08770771203092015, train acc:0.965, test acc:0.961 ===\n",
            "=== epoch:4, train loss:0.06966998995862105, train acc:0.975, test acc:0.97 ===\n",
            "=== epoch:5, train loss:0.079466962129633, train acc:0.977, test acc:0.973 ===\n",
            "=== epoch:6, train loss:0.11081197156387568, train acc:0.981, test acc:0.971 ===\n",
            "=== epoch:7, train loss:0.025746444152759777, train acc:0.982, test acc:0.978 ===\n",
            "=== epoch:8, train loss:0.020832298142524547, train acc:0.985, test acc:0.984 ===\n",
            "=== epoch:9, train loss:0.034014540137346505, train acc:0.985, test acc:0.984 ===\n",
            "=== epoch:10, train loss:0.02184144270202796, train acc:0.983, test acc:0.98 ===\n",
            "=== epoch:11, train loss:0.008452193716953861, train acc:0.988, test acc:0.986 ===\n",
            "=== epoch:12, train loss:0.09338544881257961, train acc:0.991, test acc:0.981 ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-9c4396aafb6c>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-b8b857a5df64>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# 학습 중단 신호가 반환되면 루프 탈출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-b8b857a5df64>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-59a84b6733d7>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"\"\"오차역전파법으로 기울기를 구함\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# 순전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# 역전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-59a84b6733d7>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;34m\"\"\"손실함수 값 계산\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-59a84b6733d7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;34m\"\"\"추론을 수행\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 결과에서 train_acc와 test_acc 추출\n",
        "train_acc_list = [0.925, 0.958, 0.965, 0.975, 0.977, 0.981, 0.982, 0.985, 0.985, 0.983, 0.988]\n",
        "test_acc_list = [0.924, 0.955, 0.961, 0.97, 0.973, 0.971, 0.978, 0.984, 0.984, 0.98, 0.986]\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc', marker='o')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--', marker='s')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0.9, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "R-rNcNShmoLl",
        "outputId": "578ad6f5-2917-4c6a-f1d1-48ee0faf2e58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}